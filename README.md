# Hi, I'm Jm Stilb

**Data Scientist & AI Engineer** building intelligent systems at the intersection of machine learning and production AI.

---

## About Me

I design and build AI systems that solve real problems -- from evaluation frameworks that measure what matters to production pipelines that scale. My work spans traditional machine learning foundations through modern AI engineering: RAG systems, agentic orchestration, LLM evaluation, and context engineering.

Currently based in **San Diego, CA** and open to **Data Scientist** and **AI Engineer** opportunities.

## What I'm Building

- **Human-centered AI evaluation** -- frameworks and playbooks for measuring AI system quality beyond accuracy scores
- **Production AI infrastructure** -- multi-agent systems, RAG pipelines, and context engineering at scale
- **Open-source tools** -- contributing evaluation frameworks and governance templates to the AI community

## Featured Projects

### Modern AI Engineering

| Project | Description | Stack | Tests |
|---------|-------------|-------|-------|
| [modern-rag-pipeline](https://github.com/jstilb/modern-rag-pipeline) | Production RAG with hybrid retrieval (semantic + BM25 + RRF), 4 chunking strategies, and IR evaluation metrics | Python, ChromaDB, FastAPI | 109 tests, 88% cov |
| [mcp-toolkit-server](https://github.com/jstilb/mcp-toolkit-server) | MCP server with 5 tools, 3 resources, 3 prompts, and provider dependency injection | TypeScript, MCP SDK, Zod | 64 tests, 96% cov |
| [agent-orchestrator](https://github.com/jstilb/agent-orchestrator) | Multi-agent system with state machine coordination, quality review loop, and typed message passing | Python, LangGraph, FastAPI | 31 tests, 90% cov |
| [llm-eval-framework](https://github.com/jstilb/llm-eval-framework) | LLM evaluation with 9 metrics, LLM-as-judge, pairwise comparison, and multi-format reporting | Python, FastAPI, Hypothesis | 102 tests, 85% cov |

### AI Governance & Evaluation

| Project | Description | Stack |
|---------|-------------|-------|
| [meaningful_metrics](https://github.com/jstilb/meaningful_metrics) | Open-source evaluation frameworks for human-centered metrics, AI evaluation playbooks, and governance templates | Python, LLM Evaluation, Responsible AI |

### Traditional ML & Data Science

| Project | Domain |
|---------|--------|
| [portfolio](https://github.com/jstilb/portfolio) | NLP research (bias detection, knowledge graphs), statistical inference (hypothesis testing, causal regression), big data (Spark/MapReduce), ML deployment (AKS) |

All modern AI projects include CI/CD pipelines, Docker support, architecture docs, ADRs, and mock mode for zero-dependency demos. **306 tests** across 4 repos.

## Skills Matrix

```
Domain                          Skill                        Demonstrated In
-----                           -----                        ---------------
ML & Data Science               Python                       All repos
                                Statistical Inference        covid-hypothesis, vax-travel
                                NLP / Transformers           bias-detection, knowledge-graph-qa
                                Big Data (Spark)             flight-prediction
                                Experiment Design            info-consumption-research

AI Engineering                  RAG Pipelines                modern-rag-pipeline
                                Vector Databases (ChromaDB)  modern-rag-pipeline
                                Agentic Orchestration        agent-orchestrator
                                MCP / Context Engineering    mcp-toolkit-server
                                LLM Evaluation               llm-eval-framework, meaningful_metrics
                                LLM-as-Judge                 llm-eval-framework

Infrastructure                  FastAPI                      4 repos
                                TypeScript                   mcp-toolkit-server
                                Docker / CI/CD               All modern repos
                                TDD (306 tests)              All modern repos
                                ML Deployment (AKS)          ml-sysems-engineering
```

**Machine Learning & Data Science**
Python | scikit-learn | PyTorch | TensorFlow | XGBoost | Pandas | NumPy | SQL | Statistical Inference | Experiment Design

**AI Engineering**
LLM APIs (Claude, GPT-4, Gemini) | RAG Pipelines | Agentic Orchestration | Context Engineering | Prompt Engineering | MCP (Model Context Protocol) | LLM Evaluation | Vector Databases

**Infrastructure & DevOps**
Docker | AWS | GitHub Actions | CI/CD | TypeScript | Bun | FastAPI | MLflow | Weights & Biases

## Education

**UC Berkeley** -- Master of Information and Data Science (MIDS)

## Connect

- Open to collaboration on AI evaluation, responsible AI, and production ML systems
- Interested in roles where I can build AI systems that are both powerful and trustworthy
