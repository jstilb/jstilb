# Hi, I'm Jm Stilb

**Data Scientist & AI Engineer** building intelligent systems at the intersection of machine learning and production AI.

---

## About Me

I design and build AI systems that solve real problems -- from evaluation frameworks that measure what matters to production pipelines that scale. My work spans traditional machine learning foundations through modern AI engineering: RAG systems, agentic orchestration, LLM evaluation, and context engineering.

Currently based in **San Diego, CA** and open to **Data Scientist** and **AI Engineer** opportunities.

## What I'm Building

- **Human-centered AI evaluation** -- frameworks and playbooks for measuring AI system quality beyond accuracy scores
- **Production AI infrastructure** -- multi-agent systems, RAG pipelines, and context engineering at scale
- **Open-source tools** -- contributing evaluation frameworks and governance templates to the AI community

## Featured Projects

### Modern AI Engineering

| Project | Description | Stack | Tests |
|---------|-------------|-------|-------|
| [modern-rag-pipeline](https://github.com/jstilb/modern-rag-pipeline) | Production RAG pipeline with 4 chunking strategies, hybrid retrieval (semantic + BM25 + RRF), and IR evaluation metrics | Python, LangChain, ChromaDB, FastAPI | 109 tests, 88% cov |
| [mcp-toolkit-server](https://github.com/jstilb/mcp-toolkit-server) | Model Context Protocol server with 5 tools, 3 resources, 3 prompts, and provider dependency injection | TypeScript, MCP SDK, Zod, Jest | 64 tests, 96% cov |
| [agent-orchestrator](https://github.com/jstilb/agent-orchestrator) | Multi-agent system with state machine coordination, quality review loop, and typed message passing | Python, LangGraph, FastAPI, Pydantic | 31 tests, 90% cov |
| [llm-eval-framework](https://github.com/jstilb/llm-eval-framework) | LLM evaluation with 9 custom metrics, LLM-as-judge pattern, pairwise comparison, and multi-format reporting | Python, FastAPI, Hypothesis | 102 tests, 85% cov |

### AI Governance & Evaluation

| Project | Description | Stack |
|---------|-------------|-------|
| [meaningful_metrics](https://github.com/jstilb/meaningful_metrics) | Open-source evaluation frameworks for human-centered metrics, AI evaluation playbooks, and governance templates | Python, LLM Evaluation, Responsible AI |

All projects include CI/CD pipelines, Docker support, architecture docs, ADRs, and mock mode for zero-dependency demos.

## Technical Skills

**Machine Learning & Data Science**
Python | scikit-learn | PyTorch | TensorFlow | XGBoost | Pandas | NumPy | SQL | Statistical Inference | Experiment Design

**AI Engineering**
LLM APIs (Claude, GPT-4, Gemini) | RAG Pipelines | Agentic Orchestration | Context Engineering | Prompt Engineering | MCP (Model Context Protocol) | LLM Evaluation | Vector Databases

**Infrastructure & DevOps**
Docker | AWS | GitHub Actions | CI/CD | TypeScript | Bun | Vercel | MLflow | Weights & Biases

## Education

**UC Berkeley** -- Master of Information and Data Science (MIDS)

## Connect

- Open to collaboration on AI evaluation, responsible AI, and production ML systems
- Interested in roles where I can build AI systems that are both powerful and trustworthy
